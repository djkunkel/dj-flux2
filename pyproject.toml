[project]
name = "dj-flux2"
version = "0.1.0"
description = "Minimal FLUX.2 Klein 4B inference with CUDA support"
readme = "README.md"
requires-python = ">=3.10"
authors = [
  { name = "DJ", email = "dj@example.com" }
]
license = "MIT"

dependencies = [
  "torch>=2.8.0",
  "torchvision>=0.23.0",
  "einops>=0.8.1",
  "transformers>=4.56.1",
  "safetensors>=0.4.5",
  "pillow>=10.0.0",
  "huggingface-hub>=0.36.0",
  "accelerate>=1.0.0",
  "spandrel>=0.4.0",
  "PySide6>=6.6.0",
  # triton has no official Windows wheels; triton-windows is the community port.
  # It provides the same API and is required by transformers' FP8 quantization
  # (used by the Qwen3-4B-FP8 text encoder). Not needed on Linux, which gets
  # triton directly as a torch dependency.
  "triton-windows>=3.6.0; sys_platform == 'win32'",
]

[project.scripts]
dj-flux2 = "generate_image:main"
dj-flux2-upscale = "upscale_image:main"
dj-flux2-download = "download_models:main"
dj-flux2-gui = "gui_generate:main"

[build-system]
requires = ["setuptools>=64", "wheel"]
build-backend = "setuptools.build_meta"

[tool.uv]
python-preference = "only-managed"

# On Windows, PyPI only serves CPU-only torch wheels.
# Pull CUDA-enabled wheels from the official PyTorch index instead.
[tool.uv.sources]
torch = [
  { index = "pytorch-cu128", marker = "sys_platform == 'win32'" },
]
torchvision = [
  { index = "pytorch-cu128", marker = "sys_platform == 'win32'" },
]

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[tool.setuptools]
# Include the main scripts as modules
py-modules = ["generate_image", "upscale_image", "download_models", "gui_generate", "gui_components"]
packages = []
